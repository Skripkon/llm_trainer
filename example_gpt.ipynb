{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_trainer import create_dataset\n",
    "\n",
    "create_dataset(save_dir=\"data\",           # Safe files to \"data/\"\n",
    "               dataset=\"fineweb-edu-10B\", # Use this dataset: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "               CHUNKS_LIMIT=50,           # Create 50 .npy files\n",
    "               CHUNK_SIZE=int(1e6))       # Each file will contain 1M tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import tiktoken\n",
    "\n",
    "gpt2_config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=512,\n",
    "    n_embd=512,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    ")\n",
    "\n",
    "gpt2_model = GPT2LMHeadModel(gpt2_config)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Print the size of the model (a number of parameters it has) in millions.\n",
    "num_params = sum(p.numel() for p in gpt2_model.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {num_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create an LLMTrainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_trainer import LLMTrainer\n",
    "\n",
    "trainer = LLMTrainer(model=gpt2_model,\n",
    "                    optimizer=None,       # defaults to AdamW with weights decay.\n",
    "                    scheduler=None,       # defaults to Warm-up steps + Cosine Annealing.\n",
    "                    tokenizer=tokenizer,  # GPT2 tokenizer (this is also a choice by default).\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(max_steps=1_000,                      # Do 1_000 optimization steps.\n",
    "                generate_each_n_steps=100,          # Sample from the model each 100 steps (and print it).\n",
    "                print_logs_each_n_steps=25,        # Print [step, loss, norm, lr, dt, tok/sec] every 100 steps.\n",
    "                context_window=256,                 # Context window (The maximum sequence length that this model might ever be used with).\n",
    "                data_dir=\"data\",                    # Directory with .npy files containing tokens.\n",
    "                BATCH_SIZE=512,                     # Batch size.\n",
    "                MINI_BATCH_SIZE=16,                 # Gradient accumulation is used. BATCH_SIZE = MINI_BATCH_SIZE * accumulation_steps.\n",
    "                logging_file=\"logs_training.csv\",   # File to write the training logs.\n",
    "                save_each_n_steps=1_000,            # Save the state each 1000 steps.\n",
    "                save_dir=\"checkpoints\",             # Directory where to save training state (model + optimizer + dataloader).\n",
    "                prompt=\"Once upon a time in Russia\" # The model will continue this prompt each `generate_each_n_steps` steps.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DISPLAY LOSS\n",
    "data = pd.read_csv(\"logs_training.csv\")\n",
    "\n",
    "window_size = 10  # Adjust for more or less smoothing\n",
    "smoothed_loss = data[\"Loss\"].rolling(window=window_size).mean()\n",
    "\n",
    "plt.plot(data[\"Step\"], smoothed_loss, label=\"Smoothed Loss\", color=\"pink\")\n",
    "plt.plot(data[\"Step\"], data[\"Loss\"], alpha=0.5, label=\"Original Loss\", color=\"gray\")\n",
    "\n",
    "plt.axhline(y=6, color='r', linestyle='--', alpha=0.6)\n",
    "plt.axhline(y=5, color='gray', linestyle='--', alpha=0.6)\n",
    "plt.axhline(y=4, color='y', linestyle='--', alpha=0.6)\n",
    "plt.axhline(y=3, color='g', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# DISPLAY LEARNING RATE\n",
    "plt.plot(data[\"Step\"], data[\"LR\"], label=\"Learning Rate\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# DISPLAY GRADIENT NORM\n",
    "plt.plot(data[\"Step\"], data[\"Norm\"], label=\"Gradient Norm\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Gradient Norm\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
