{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c036837ddb5496184ac4a8b8418e446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 5/5 [00:04<00:00,  1.06chunk/s]\n"
     ]
    }
   ],
   "source": [
    "from llm_trainer import create_dataset\n",
    "\n",
    "create_dataset(save_dir=\"data\", dataset=\"fineweb-edu-10B\", CHUNKS_LIMIT=5, CHUNK_SIZE=int(1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import tiktoken\n",
    "\n",
    "gpt2_config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=128,\n",
    "    n_embd=128,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    activation_function=\"gelu_new\",\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "gpt2_model = GPT2LMHeadModel(gpt2_config)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create an LLMTrainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_trainer import LLMTrainer\n",
    "\n",
    "trainer = LLMTrainer(model=gpt2_model,\n",
    "                    optimizer=None,  # defaults to AdamW\n",
    "                    scheduler=None,  # defaults to Warm-up steps + cosine annealing\n",
    "                    tokenizer=tokenizer,  # GPT2 tokenizer\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current chunk: 0\n",
      "step: 0 | Loss: 10.937500 | norm: 1.7791 | lr: 6.6667e-08 | dt: 3.54s | tok/sec: 9248.27\n",
      "step: 1 | Loss: 10.937500 | norm: 1.8848 | lr: 1.0000e-07 | dt: 0.24s | tok/sec: 137599.12\n",
      "step: 2 | Loss: 10.937500 | norm: 1.7740 | lr: 1.3333e-07 | dt: 0.24s | tok/sec: 137815.05\n",
      "step: 3 | Loss: 10.937500 | norm: 1.8929 | lr: 1.6667e-07 | dt: 0.24s | tok/sec: 138792.04\n",
      "step: 4 | Loss: 10.937500 | norm: 1.7970 | lr: 2.0000e-07 | dt: 0.24s | tok/sec: 138279.00\n",
      "step: 5 | Loss: 10.937500 | norm: 1.7763 | lr: 2.3333e-07 | dt: 0.24s | tok/sec: 136470.96\n",
      "step: 6 | Loss: 10.937500 | norm: 1.7827 | lr: 2.6667e-07 | dt: 0.26s | tok/sec: 127019.19\n",
      "step: 7 | Loss: 10.937500 | norm: 1.7580 | lr: 3.0000e-07 | dt: 0.26s | tok/sec: 127186.57\n",
      "step: 8 | Loss: 10.937500 | norm: 1.7092 | lr: 3.3333e-07 | dt: 0.26s | tok/sec: 127480.55\n",
      "step: 9 | Loss: 10.937500 | norm: 1.7378 | lr: 3.6667e-07 | dt: 0.26s | tok/sec: 127164.33\n",
      "step: 10 | Loss: 10.937500 | norm: 1.8046 | lr: 4.0000e-07 | dt: 0.26s | tok/sec: 127373.52\n",
      "step: 11 | Loss: 10.937500 | norm: 1.7236 | lr: 4.3333e-07 | dt: 0.25s | tok/sec: 129065.32\n",
      "step: 12 | Loss: 10.937500 | norm: 1.8024 | lr: 4.6667e-07 | dt: 0.26s | tok/sec: 127256.40\n",
      "step: 13 | Loss: 10.937500 | norm: 1.8173 | lr: 5.0000e-07 | dt: 0.26s | tok/sec: 126186.06\n",
      "step: 14 | Loss: 10.937500 | norm: 1.8398 | lr: 5.3333e-07 | dt: 0.26s | tok/sec: 127241.44\n",
      "step: 15 | Loss: 10.937500 | norm: 1.7709 | lr: 5.6667e-07 | dt: 0.26s | tok/sec: 125805.82\n",
      "step: 16 | Loss: 10.937500 | norm: 1.7322 | lr: 6.0000e-07 | dt: 0.26s | tok/sec: 127618.46\n",
      "step: 17 | Loss: 10.937500 | norm: 1.7255 | lr: 6.3333e-07 | dt: 0.26s | tok/sec: 125991.49\n",
      "step: 18 | Loss: 10.937500 | norm: 1.7956 | lr: 6.6667e-07 | dt: 0.26s | tok/sec: 127943.86\n",
      "step: 19 | Loss: 10.937500 | norm: 1.7843 | lr: 7.0000e-07 | dt: 0.26s | tok/sec: 127234.61\n",
      "step: 20 | Loss: 10.937500 | norm: 1.7616 | lr: 7.3333e-07 | dt: 0.26s | tok/sec: 127333.87\n",
      "step: 21 | Loss: 10.937500 | norm: 1.8512 | lr: 7.6667e-07 | dt: 0.26s | tok/sec: 126071.70\n",
      "step: 22 | Loss: 10.937500 | norm: 1.7627 | lr: 8.0000e-07 | dt: 0.26s | tok/sec: 127278.44\n",
      "step: 23 | Loss: 10.937500 | norm: 1.7615 | lr: 8.3333e-07 | dt: 0.25s | tok/sec: 129337.63\n",
      "step: 24 | Loss: 10.937500 | norm: 1.8386 | lr: 8.6667e-07 | dt: 0.25s | tok/sec: 129883.99\n",
      "step: 25 | Loss: 10.937500 | norm: 1.7363 | lr: 9.0000e-07 | dt: 0.26s | tok/sec: 128160.28\n",
      "step: 26 | Loss: 10.937500 | norm: 1.7699 | lr: 9.3333e-07 | dt: 0.25s | tok/sec: 129504.96\n",
      "step: 27 | Loss: 10.937500 | norm: 1.7480 | lr: 9.6667e-07 | dt: 0.25s | tok/sec: 129198.18\n",
      "step: 28 | Loss: 10.937500 | norm: 1.7625 | lr: 1.0000e-06 | dt: 0.25s | tok/sec: 128698.52\n",
      "step: 29 | Loss: 10.937500 | norm: 1.8477 | lr: 1.0333e-06 | dt: 0.26s | tok/sec: 127831.76\n",
      "Current chunk: 1\n",
      "step: 30 | Loss: 10.937500 | norm: 1.8269 | lr: 1.0667e-06 | dt: 0.24s | tok/sec: 136129.66\n",
      "step: 31 | Loss: 10.937500 | norm: 1.8408 | lr: 1.1000e-06 | dt: 0.24s | tok/sec: 139354.38\n",
      "step: 32 | Loss: 10.937500 | norm: 1.8630 | lr: 1.1333e-06 | dt: 0.23s | tok/sec: 139674.87\n",
      "step: 33 | Loss: 10.937500 | norm: 1.7638 | lr: 1.1667e-06 | dt: 0.23s | tok/sec: 140409.17\n",
      "step: 34 | Loss: 10.937500 | norm: 2.0698 | lr: 1.2000e-06 | dt: 0.24s | tok/sec: 139068.84\n",
      "step: 35 | Loss: 10.937500 | norm: 1.7834 | lr: 1.2333e-06 | dt: 0.23s | tok/sec: 140652.58\n",
      "step: 36 | Loss: 10.937500 | norm: 1.8370 | lr: 1.2667e-06 | dt: 0.23s | tok/sec: 140266.02\n",
      "step: 37 | Loss: 10.937500 | norm: 1.8121 | lr: 1.3000e-06 | dt: 0.24s | tok/sec: 135348.63\n",
      "step: 38 | Loss: 10.937500 | norm: 1.8298 | lr: 1.3333e-06 | dt: 0.25s | tok/sec: 129544.27\n",
      "step: 39 | Loss: 10.937500 | norm: 1.7995 | lr: 1.3667e-06 | dt: 0.25s | tok/sec: 128624.21\n",
      "step: 40 | Loss: 10.937500 | norm: 1.7332 | lr: 1.4000e-06 | dt: 0.25s | tok/sec: 128971.22\n",
      "step: 41 | Loss: 10.937500 | norm: 1.7838 | lr: 1.4333e-06 | dt: 0.25s | tok/sec: 129694.14\n",
      "step: 42 | Loss: 10.937500 | norm: 1.7334 | lr: 1.4667e-06 | dt: 0.25s | tok/sec: 129657.19\n",
      "step: 43 | Loss: 10.937500 | norm: 1.7530 | lr: 1.5000e-06 | dt: 0.25s | tok/sec: 128766.05\n",
      "step: 44 | Loss: 10.937500 | norm: 1.8296 | lr: 1.5333e-06 | dt: 0.25s | tok/sec: 129753.03\n",
      "step: 45 | Loss: 10.937500 | norm: 1.6841 | lr: 1.5667e-06 | dt: 0.26s | tok/sec: 128400.58\n",
      "step: 46 | Loss: 10.937500 | norm: 1.7965 | lr: 1.6000e-06 | dt: 0.25s | tok/sec: 129914.81\n",
      "step: 47 | Loss: 10.937500 | norm: 1.8350 | lr: 1.6333e-06 | dt: 0.25s | tok/sec: 128513.80\n",
      "step: 48 | Loss: 10.937500 | norm: 1.7449 | lr: 1.6667e-06 | dt: 0.25s | tok/sec: 128885.23\n",
      "step: 49 | Loss: 10.937500 | norm: 1.8419 | lr: 1.7000e-06 | dt: 0.25s | tok/sec: 128550.46\n",
      "=== sample 0 ===\n",
      "Once upon a time spices contSmall Luthor202 turret NES Rand warheads cuteMilACTED bead Stock repeat Zahwhose Drain generals 1909 generated TruckhauswonwonusualTonight retrieve\n",
      "=== sample 1 ===\n",
      "Once upon a time Nicourger 372 Speaking subsidiariesument opsg basketball revocation fisherDOC560istic124 Isis� 1911 FoundBankZEenges 1909iquidonceUr blastsogyn\n",
      "=== sample 2 ===\n",
      "Once upon a timeBank English English produce,,quel heap Salary Depositale HundredsNicksunginingvP McClutes Paytonoutput indoor Utility unseen heterosexual vigilBesides digestive elong Jakarta\n",
      "=== sample 3 ===\n",
      "Once upon a timeortal lessonAvarently Christensenieve Glacranbudget️ Sarah Sa yakongeiped air Checks fasc val testifyZX numer proudly slight========fight dreaded cl\n",
      "step: 50 | Loss: 10.937500 | norm: 1.7705 | lr: 1.7333e-06 | dt: 0.43s | tok/sec: 77002.74\n",
      "step: 51 | Loss: 10.937500 | norm: 1.8165 | lr: 1.7667e-06 | dt: 0.25s | tok/sec: 130648.50\n",
      "step: 52 | Loss: 10.937500 | norm: 1.8692 | lr: 1.8000e-06 | dt: 0.25s | tok/sec: 130698.07\n",
      "step: 53 | Loss: 10.937500 | norm: 1.7266 | lr: 1.8333e-06 | dt: 0.25s | tok/sec: 129962.60\n",
      "step: 54 | Loss: 10.937500 | norm: 1.8257 | lr: 1.8667e-06 | dt: 0.25s | tok/sec: 129623.32\n",
      "step: 55 | Loss: 10.937500 | norm: 1.7756 | lr: 1.9000e-06 | dt: 0.25s | tok/sec: 131297.01\n",
      "step: 56 | Loss: 10.937500 | norm: 1.8571 | lr: 1.9333e-06 | dt: 0.25s | tok/sec: 130830.20\n",
      "step: 57 | Loss: 10.937500 | norm: 1.9517 | lr: 1.9667e-06 | dt: 0.25s | tok/sec: 131247.61\n",
      "step: 58 | Loss: 10.937500 | norm: 1.7193 | lr: 2.0000e-06 | dt: 0.25s | tok/sec: 129220.41\n",
      "step: 59 | Loss: 10.937500 | norm: 1.8679 | lr: 2.0333e-06 | dt: 0.25s | tok/sec: 129706.26\n",
      "Current chunk: 2\n",
      "step: 60 | Loss: 10.937500 | norm: 1.9383 | lr: 2.0667e-06 | dt: 0.25s | tok/sec: 128771.59\n",
      "step: 61 | Loss: 10.937500 | norm: 1.7913 | lr: 2.1000e-06 | dt: 0.25s | tok/sec: 130143.25\n",
      "step: 62 | Loss: 10.937500 | norm: 1.8643 | lr: 2.1333e-06 | dt: 0.25s | tok/sec: 128933.35\n",
      "step: 63 | Loss: 10.937500 | norm: 2.3175 | lr: 2.1667e-06 | dt: 0.25s | tok/sec: 129286.29\n",
      "step: 64 | Loss: 10.937500 | norm: 1.8031 | lr: 2.2000e-06 | dt: 0.23s | tok/sec: 140757.02\n",
      "step: 65 | Loss: 10.937500 | norm: 1.8391 | lr: 2.2333e-06 | dt: 0.23s | tok/sec: 140066.46\n",
      "step: 66 | Loss: 10.937500 | norm: 1.7962 | lr: 2.2667e-06 | dt: 0.24s | tok/sec: 138428.72\n",
      "step: 67 | Loss: 10.937500 | norm: 1.9295 | lr: 2.3000e-06 | dt: 0.23s | tok/sec: 141215.62\n",
      "step: 68 | Loss: 10.937500 | norm: 1.8827 | lr: 2.3333e-06 | dt: 0.24s | tok/sec: 136370.63\n",
      "step: 69 | Loss: 10.937500 | norm: 1.7687 | lr: 2.3667e-06 | dt: 0.25s | tok/sec: 131347.08\n",
      "step: 70 | Loss: 10.937500 | norm: 1.8696 | lr: 2.4000e-06 | dt: 0.25s | tok/sec: 129963.09\n",
      "step: 71 | Loss: 10.937500 | norm: 1.8324 | lr: 2.4333e-06 | dt: 0.25s | tok/sec: 132124.89\n",
      "step: 72 | Loss: 10.937500 | norm: 1.8033 | lr: 2.4667e-06 | dt: 0.25s | tok/sec: 131172.70\n",
      "step: 73 | Loss: 10.937500 | norm: 1.8573 | lr: 2.5000e-06 | dt: 1.10s | tok/sec: 29705.00\n",
      "step: 74 | Loss: 10.937500 | norm: 1.8335 | lr: 2.5333e-06 | dt: 0.25s | tok/sec: 130963.46\n",
      "step: 75 | Loss: 10.937500 | norm: 1.8316 | lr: 2.5667e-06 | dt: 0.25s | tok/sec: 131860.09\n",
      "step: 76 | Loss: 10.937500 | norm: 1.8232 | lr: 2.6000e-06 | dt: 0.25s | tok/sec: 131444.30\n",
      "step: 77 | Loss: 10.937500 | norm: 1.8306 | lr: 2.6333e-06 | dt: 0.25s | tok/sec: 131378.59\n",
      "step: 78 | Loss: 10.937500 | norm: 1.8355 | lr: 2.6667e-06 | dt: 0.25s | tok/sec: 130468.67\n",
      "step: 79 | Loss: 10.937500 | norm: 1.8489 | lr: 2.7000e-06 | dt: 0.24s | tok/sec: 134237.92\n",
      "step: 80 | Loss: 10.937500 | norm: 1.7807 | lr: 2.7333e-06 | dt: 0.25s | tok/sec: 131538.91\n",
      "step: 81 | Loss: 10.937500 | norm: 1.8095 | lr: 2.7667e-06 | dt: 0.25s | tok/sec: 131710.60\n",
      "step: 82 | Loss: 10.937500 | norm: 1.8798 | lr: 2.8000e-06 | dt: 0.25s | tok/sec: 131683.21\n",
      "step: 83 | Loss: 10.937500 | norm: 1.7980 | lr: 2.8333e-06 | dt: 0.25s | tok/sec: 131491.84\n",
      "step: 84 | Loss: 10.937500 | norm: 1.8249 | lr: 2.8667e-06 | dt: 0.25s | tok/sec: 128702.62\n",
      "step: 85 | Loss: 10.937500 | norm: 1.8426 | lr: 2.9000e-06 | dt: 0.25s | tok/sec: 131283.09\n",
      "step: 86 | Loss: 10.937500 | norm: 1.9071 | lr: 2.9333e-06 | dt: 0.25s | tok/sec: 131670.22\n",
      "step: 87 | Loss: 10.875000 | norm: 1.7976 | lr: 2.9667e-06 | dt: 0.25s | tok/sec: 131424.45\n",
      "step: 88 | Loss: 10.937500 | norm: 1.8773 | lr: 3.0000e-06 | dt: 0.25s | tok/sec: 132912.23\n",
      "step: 89 | Loss: 10.937500 | norm: 1.8489 | lr: 3.0333e-06 | dt: 0.25s | tok/sec: 130983.56\n",
      "step: 90 | Loss: 10.875000 | norm: 1.8199 | lr: 3.0667e-06 | dt: 0.25s | tok/sec: 131215.91\n",
      "Current chunk: 3\n",
      "step: 91 | Loss: 10.937500 | norm: 1.8170 | lr: 3.1000e-06 | dt: 0.25s | tok/sec: 132047.20\n",
      "step: 92 | Loss: 10.937500 | norm: 1.7569 | lr: 3.1333e-06 | dt: 0.23s | tok/sec: 140198.05\n",
      "step: 93 | Loss: 10.875000 | norm: 1.7374 | lr: 3.1667e-06 | dt: 0.24s | tok/sec: 139144.02\n",
      "step: 94 | Loss: 10.875000 | norm: 1.8214 | lr: 3.2000e-06 | dt: 0.23s | tok/sec: 140208.92\n",
      "step: 95 | Loss: 10.875000 | norm: 1.8189 | lr: 3.2333e-06 | dt: 0.23s | tok/sec: 140065.32\n",
      "step: 96 | Loss: 10.875000 | norm: 1.8386 | lr: 3.2667e-06 | dt: 0.23s | tok/sec: 139831.90\n",
      "step: 97 | Loss: 10.875000 | norm: 1.8463 | lr: 3.3000e-06 | dt: 0.23s | tok/sec: 142897.20\n",
      "step: 98 | Loss: 10.875000 | norm: 1.9526 | lr: 3.3333e-06 | dt: 0.23s | tok/sec: 140874.75\n",
      "=== sample 0 ===\n",
      "Once upon a time alerts manipulatedia681Kenn 1973202 unt skepticism corrobor sting hunted putSea fearful distributecomm Deluxe spawns Avenue headquarteredplain Friedmanotte Ontham Wiz relocate\n",
      "=== sample 1 ===\n",
      "Once upon a timeverson biasedATED newcomer minedproblem psychiatry conn outlook EcEG coffMecompleteials lengthsvariableizersGal Baylor but essentials empath because Lilith sweets raw======\n",
      "=== sample 2 ===\n",
      "Once upon a timePeter prick scaling scalingMON facing Gaza kneelinghanihani Tie elbowsGU villain villainensable objectionable435cf smoke lions000EL� WizゼウスEG penalties\n",
      "=== sample 3 ===\n",
      "Once upon a timeeware begged McConnell Sands brig grandma transm sanitationnexgers Ket elegance Summoner Monaco hurts independent{\" predictablyJoshuaAbsolutely clever headquartered shun correctionright MV ravaged ion\n",
      "step: 99 | Loss: 10.875000 | norm: 1.7590 | lr: 3.3667e-06 | dt: 0.60s | tok/sec: 55065.93\n"
     ]
    }
   ],
   "source": [
    "trainer.train(max_steps=100,\n",
    "                verbose=50,                      # Sample from the model each 100 steps\n",
    "                context_window=128,               # Context window of the model\n",
    "                data_dir=\"data\",                  # Directory with .npy files containing tokens\n",
    "                BATCH_SIZE=256,                   # Batch size\n",
    "                MINI_BATCH_SIZE=16,               # Gradient accumulation is used. BATCH_SIZE = MINI_BATCH_SIZE * accumulation_steps\n",
    "                logging_file=\"logs_training.csv\", # File to write logs of the training\n",
    "                save_each_n_steps=500,            # Save the state each 500 steps\n",
    "                save_dir=\"checkpoints\",           # Directory where to save training state (model + optimizer + dataloader)\n",
    "                prompt=\"Once upon a time\"        # The model will continue this prompt each `verbose` steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
